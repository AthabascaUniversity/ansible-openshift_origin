---
### node.yml - OpenShift Origin Node Ansible Playbook of Plays
#
# Ansible playbook to deploy OpenShift Origin Node nightly builds
#
# This is the OpenShift Origin Node ansible playbook, it contains multiple 
# plays as can be found below. Each one can be run indendently of one another
# by utilizing the ansible tags feature, an example of running only the plays
# tagged with selinux and named is as follows:
#
#       $ ansible-playbook node.yml -t selinux,postinstallconf
#
#   This is for those who want to make modifications and only run the play
# correlated to that change. Also, without the added overhead of a fanned out 
# directory structure because at this time it's not quite a large enough
# playbook that it should be necessary to do so.
#
#
#

- name: Post-Install Configure of OpenShift Origin Node
  hosts: nodes
  user: root
  vars_files:
    - varfiles/origin_vars.yml
  tasks:
    - name: node.conf
      template: src=templates/node/node.conf.j2
                dest=/etc/openshift/node.conf
                owner=root group=root mode=0644
    - name: openshift_origin_node_severname.conf
      template: src=templates/node/openshift-origin-node_servername.conf.j2
                dest=/etc/httpd/conf.d/000001_openshift_origin_node_servername.conf
                owner=root group=root mode=0644
    - name: mcollective server config
      template: src=templates/mcollective-server.cfg.j2
                dest=/etc/mcollective/server.cfg
                owner=root group=root mode=0644
    - name: mcollective client config
      template: src=templates/mcollective-client.cfg.j2
                dest=/etc/mcollective/client.cfg
                owner=root group=root mode=0644
    - name: Find root mount point of gear dir
      shell: df -P /var/lib/openshift | tail -1 | awk '{ print $6 }'
      register: gear_root_mount
    - name: Verify origin-node-util installed for quota init
      yum: pkg=openshift-origin-node-util state=installed
    - name: Initialize quota db
      shell: oo-init-quota creates=${gear_root_mount.stdout}/aquota.user
    - name: openshift node pam sshd config
      template: src=templates/node/pam.sshd-fedora.j2
                dest=/etc/pam.d/sshd
                owner=root group=root mode=0644
    - name: openshift node pam runuser
      copy: src=files/node/pam.runuser-fedora
            dest=/etc/pam.d/runuser owner=root group=root mode=0644
    - name: openshift node pam runuser-l
      copy: src=files/node/pam.runuser-l-fedora
            dest=/etc/pam.d/runuser-l owner=root group=root mode=0644
    - name: openshift node pam su
      copy: src=files/node/pam.su-fedora
            dest=/etc/pam.d/su owner=root group=root mode=0644
    - name: openshift node pam system-auth-ac
      copy: src=files/node/pam.system-auth-ac-fedora
            dest=/etc/pam.d/system-auth-ac owner=root group=root mode=0644
    - name: openshift node pam-namespace sandbox.conf
      template: src=templates/node/namespace-d-sandbox.conf.j2
            dest=/etc/security/namespace.d/sandbox.conf 
            owner=root group=root mode=0644
    - name: openshift node pam-namespace tmp.conf
      template: src=templates/node/namespace-d-tmp.conf.j2
            dest=/etc/security/namespace.d/tmp.conf 
            owner=root group=root mode=0644
    - name: openshift node pam-namespace vartmp.conf
      template: src=templates/node/namespace-d-vartmp.conf.j2
            dest=/etc/security/namespace.d/vartmp.conf 
            owner=root group=root mode=0644
    - name: openshift sysctl tweaks
      script: scripts/node/node_sysctl.sh
    - name: openshift node sshd config
      copy: src=files/node/sshd_config dest=/etc/ssh/sshd_config
            owner=root group=root mode=0600
    - name: broker and console route for node
      copy: src=files/node/node_routes.txt
            dest=/tmp/nodes.broker_routes.txt
            owner=root group=root mode=0644
    - name: regen node routes
      script: scripts/node/regen_node_routes.sh 
    - name: Restore SELinux context $gear_root_dir
      command: $restorecon -Rv $gear_root_dir
    - name: Restore SELinux context $gear_httpd_dir
      command: $restorecon -Rv $gear_httpd_dir

- name: Enable Daemons
  hosts: nodes
  user: root
  vars_files:
    - varfiles/origin_vars.yml
  tasks:
    - name: Verify libcgroup-tools are installed for cgconfig/cgred
      yum: pkg=libcgroup-tools state=installed
    - name: start/enable cgconfig service
      service: name=cgconfig state=started enabled=yes
      when_string: $configure_cgroups == 'true'
    - name: start/enable cgred service
      service: name=cgred state=started enabled=yes
      when_string: $configure_cgroups == 'true'
    - name: Verify origin-node rubygem installed for openshift-cgroups
      yum: pkg=rubygem-openshift-origin-node state=installed
    - name: start/enable openshift-cgroups service
      service: name=openshift-cgroups state=started enabled=yes
      when_string: $configure_cgroups == 'true'
    - name: Verify origin-port-proxy installed
      yum: pkg=openshift-origin-port-proxy state=installed
    - name: start/enable openshift-port-proxy service
      service: name=openshift-port-proxy state=started enabled=yes
      when_string: $configure_cgroups == 'true'
    - name: start/enable crond
      service: name=crond state=started enabled=yes
    - name: start/enable openshift-gears
      service: name=openshift-gears state=started enabled=yes
    - name: start/enable openshift-node-web-proxy
      service: name=openshift-node-web-proxy state=started enabled=yes
    - name: start/enable activemq
      service: name=activemq state=started enabled=yes
    - name: start/enable mcollective
      service: name=mcollective state=started enabled=yes
  tags:
    - daemons
